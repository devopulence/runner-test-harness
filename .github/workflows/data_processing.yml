name: Data Processing & Volume Test

on:
  workflow_dispatch:
    inputs:
      test_id:
        description: 'Unique test identifier'
        required: true
        type: string
      data_volume_mb:
        description: 'Total data volume to process (MB)'
        required: false
        default: '100'
        type: string
      chunk_size_mb:
        description: 'Size of each data chunk (MB)'
        required: false
        default: '10'
        type: string
      processing_type:
        description: 'Type of processing to perform'
        required: false
        default: 'comprehensive'
        type: choice
        options:
          - simple
          - moderate
          - comprehensive

jobs:
  data-generation:
    runs-on: ubuntu-latest
    name: Generate Data - ${{ inputs.test_id }}
    outputs:
      chunk_count: ${{ steps.calculate.outputs.chunk_count }}

    steps:
      - name: ðŸ“ Test Configuration
        run: |
          echo "Data Processing Test Configuration"
          echo "==================================="
          echo "Test ID: ${{ inputs.test_id }}"
          echo "Data Volume: ${{ inputs.data_volume_mb }}MB"
          echo "Chunk Size: ${{ inputs.chunk_size_mb }}MB"
          echo "Processing Type: ${{ inputs.processing_type }}"

      - name: ðŸ”¢ Calculate Chunks
        id: calculate
        run: |
          CHUNK_COUNT=$(( ${{ inputs.data_volume_mb }} / ${{ inputs.chunk_size_mb }} ))
          echo "chunk_count=${CHUNK_COUNT}" >> $GITHUB_OUTPUT
          echo "Will create ${CHUNK_COUNT} chunks of ${{ inputs.chunk_size_mb }}MB each"

      - name: ðŸ“ Generate Data Chunks
        run: |
          echo "Generating test data chunks..."
          mkdir -p data_chunks

          CHUNK_COUNT=$(( ${{ inputs.data_volume_mb }} / ${{ inputs.chunk_size_mb }} ))

          for i in $(seq 1 ${CHUNK_COUNT}); do
            echo "Creating chunk ${i}/${CHUNK_COUNT}..."
            dd if=/dev/urandom of=data_chunks/chunk_${i}.dat bs=1M count=${{ inputs.chunk_size_mb }} 2>/dev/null

            # Add some structure to the data
            echo "CHUNK_ID=${i}" >> data_chunks/chunk_${i}.meta
            echo "SIZE_MB=${{ inputs.chunk_size_mb }}" >> data_chunks/chunk_${i}.meta
            echo "TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> data_chunks/chunk_${i}.meta
            echo "TEST_ID=${{ inputs.test_id }}" >> data_chunks/chunk_${i}.meta
          done

          echo "Generated ${CHUNK_COUNT} data chunks"
          ls -lh data_chunks/

      - name: ðŸ“¤ Upload Data Chunks
        uses: actions/upload-artifact@v3
        with:
          name: data-chunks-${{ inputs.test_id }}
          path: data_chunks/
          retention-days: 1

  data-processing:
    needs: data-generation
    runs-on: ubuntu-latest
    name: Process Data - ${{ inputs.test_id }}

    steps:
      - name: ðŸ“¥ Download Data Chunks
        uses: actions/download-artifact@v3
        with:
          name: data-chunks-${{ inputs.test_id }}
          path: ./data_chunks

      - name: ðŸ“Š Analyze Data Volume
        run: |
          echo "Analyzing data volume..."

          # Count files and total size
          FILE_COUNT=$(ls -1 data_chunks/*.dat | wc -l)
          TOTAL_SIZE=$(du -sh data_chunks | awk '{print $1}')

          echo "Files to process: ${FILE_COUNT}"
          echo "Total data size: ${TOTAL_SIZE}"

          # Memory and disk check
          echo "System resources:"
          free -h
          df -h .

      - name: ðŸ”„ Process Data Chunks
        run: |
          echo "Processing data based on type: ${{ inputs.processing_type }}"
          mkdir -p processed_data

          case "${{ inputs.processing_type }}" in
            simple)
              echo "Simple processing: checksums only"
              for file in data_chunks/*.dat; do
                basename=$(basename $file)
                sha256sum $file > processed_data/${basename}.sha256
              done
              ;;

            moderate)
              echo "Moderate processing: compression and checksums"
              for file in data_chunks/*.dat; do
                basename=$(basename $file)
                echo "Processing ${basename}..."

                # Compress
                gzip -c $file > processed_data/${basename}.gz

                # Checksum both original and compressed
                sha256sum $file > processed_data/${basename}.sha256
                sha256sum processed_data/${basename}.gz >> processed_data/${basename}.sha256
              done
              ;;

            comprehensive)
              echo "Comprehensive processing: multiple transformations"
              for file in data_chunks/*.dat; do
                basename=$(basename $file)
                echo "Comprehensively processing ${basename}..."

                # Multiple compression algorithms
                gzip -c $file > processed_data/${basename}.gz
                bzip2 -c $file > processed_data/${basename}.bz2
                xz -c $file > processed_data/${basename}.xz

                # Checksums
                sha256sum $file > processed_data/${basename}.checksums
                sha512sum $file >> processed_data/${basename}.checksums
                md5sum $file >> processed_data/${basename}.checksums

                # Base64 encoding (first 1MB only to save time)
                head -c 1048576 $file | base64 > processed_data/${basename}.b64

                # Statistics
                wc -c $file > processed_data/${basename}.stats
                echo "Compression ratios:" >> processed_data/${basename}.stats
                echo "GZIP: $(stat -c%s processed_data/${basename}.gz)" >> processed_data/${basename}.stats
                echo "BZIP2: $(stat -c%s processed_data/${basename}.bz2)" >> processed_data/${basename}.stats
                echo "XZ: $(stat -c%s processed_data/${basename}.xz)" >> processed_data/${basename}.stats
              done
              ;;
          esac

          echo "Processing complete"
          ls -lh processed_data/

      - name: ðŸ”€ Data Aggregation
        run: |
          echo "Aggregating processed data..."

          # Combine all checksums
          cat processed_data/*.sha256 2>/dev/null > all_checksums.txt || echo "No SHA256 files"
          cat processed_data/*.checksums 2>/dev/null >> all_checksums.txt || echo "No checksum files"

          # Create summary
          cat << EOF > processing_summary.json
          {
            "test_id": "${{ inputs.test_id }}",
            "data_volume_mb": ${{ inputs.data_volume_mb }},
            "chunk_size_mb": ${{ inputs.chunk_size_mb }},
            "chunk_count": ${{ needs.data-generation.outputs.chunk_count }},
            "processing_type": "${{ inputs.processing_type }}",
            "processed_files": $(ls -1 processed_data/ | wc -l),
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

          cat processing_summary.json | jq '.'

      - name: ðŸ“ˆ Calculate Metrics
        run: |
          echo "Calculating processing metrics..."

          # Original vs processed size comparison
          ORIGINAL_SIZE=$(du -sb data_chunks/*.dat | awk '{sum+=$1} END {print sum}')
          PROCESSED_SIZE=$(du -sb processed_data/ | awk '{print $1}')

          echo "Original data size: ${ORIGINAL_SIZE} bytes"
          echo "Processed data size: ${PROCESSED_SIZE} bytes"

          if [ "${{ inputs.processing_type }}" != "simple" ]; then
            # Calculate average compression ratio for gz files
            if ls processed_data/*.gz 1> /dev/null 2>&1; then
              GZ_SIZE=$(du -sb processed_data/*.gz | awk '{sum+=$1} END {print sum}')
              COMPRESSION_RATIO=$(echo "scale=2; ${ORIGINAL_SIZE} / ${GZ_SIZE}" | bc)
              echo "Compression ratio (gzip): ${COMPRESSION_RATIO}:1"
            fi
          fi

      - name: ðŸ—ƒï¸ Create Archive
        run: |
          echo "Creating final archive..."

          tar czf processed_archive.tar.gz processed_data/
          echo "Archive size: $(ls -lh processed_archive.tar.gz | awk '{print $5}')"

      - name: ðŸ“¤ Upload Results
        uses: actions/upload-artifact@v3
        with:
          name: processing-results-${{ inputs.test_id }}
          path: |
            processing_summary.json
            all_checksums.txt
            processed_archive.tar.gz

      - name: âœ… Processing Complete
        run: |
          echo "========================================="
          echo "DATA PROCESSING TEST COMPLETED"
          echo "========================================="
          echo "Test ID: ${{ inputs.test_id }}"
          echo "Data Volume: ${{ inputs.data_volume_mb }}MB"
          echo "Processing Type: ${{ inputs.processing_type }}"
          echo "Chunks Processed: ${{ needs.data-generation.outputs.chunk_count }}"
          echo "Status: SUCCESS"
          echo "========================================="

  stress-test:
    needs: data-generation
    runs-on: ubuntu-latest
    name: Volume Stress Test - ${{ inputs.test_id }}
    if: ${{ inputs.data_volume_mb >= 500 }}  # Only run for large volumes

    steps:
      - name: ðŸ“¥ Download All Data
        uses: actions/download-artifact@v3
        with:
          name: data-chunks-${{ inputs.test_id }}
          path: ./stress_test

      - name: ðŸ’¾ Memory Stress Test
        run: |
          echo "Performing memory stress test with large data..."

          # Try to load all data into memory (carefully)
          total_size=$(du -sb stress_test/*.dat | awk '{sum+=$1} END {print sum}')
          available_mem=$(free -b | awk '/^Mem:/{print $7}')

          echo "Total data size: $((total_size / 1048576))MB"
          echo "Available memory: $((available_mem / 1048576))MB"

          if [ $total_size -lt $available_mem ]; then
            echo "Attempting to load all data into memory..."
            cat stress_test/*.dat > /dev/null
            echo "Memory load test completed"
          else
            echo "Data too large for available memory, skipping memory test"
          fi

      - name: ðŸ”„ Concurrent I/O Stress
        run: |
          echo "Performing concurrent I/O stress test..."

          # Launch multiple concurrent I/O operations
          for i in {1..5}; do
            (
              for file in stress_test/*.dat; do
                cp $file /tmp/copy_${i}_$(basename $file) 2>/dev/null
                rm -f /tmp/copy_${i}_$(basename $file)
              done
            ) &
          done

          wait
          echo "Concurrent I/O test completed"

      - name: âœ… Stress Test Complete
        run: |
          echo "Volume stress test completed for ${{ inputs.data_volume_mb }}MB"