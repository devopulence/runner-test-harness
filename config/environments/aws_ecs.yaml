# AWS ECS Environment Configuration
# This configuration is for testing the harness on AWS ECS Fargate runners

environment:
  name: aws-ecs
  description: AWS ECS Fargate GitHub Runners
  type: test
  region: us-east-1

github:
  owner: devopulence  # Changed to lowercase
  repo: pythonProject
  ref: main
  # Runner labels must match your ECS task definition
  runner_labels:
    - self-hosted
    - ecs-fargate
    - aws
    - linux

runners:
  count: 4  # Hard limit - exactly 4 runners
  type: fargate
  cpu: 1024    # 1 vCPU
  memory: 2048 # 2GB RAM

  # Expected startup time for runners (seconds)
  startup_time: 60

  # Maximum job duration before timeout (seconds)
  max_job_duration: 3600  # 1 hour

workflows:
  directory: .github/workflows

  # Primary workflow configuration - CHANGE THIS FOR DIFFERENT ENVIRONMENTS
  # For OpenShift: change file to your actual workflow (e.g., openshift_pipeline.yml)
  primary:
    name: default                    # Logical name used in test profiles
    file: build_job.yml              # Actual workflow file to dispatch
    description: Primary test workflow

  # Workload types supported by the primary workflow
  # These map to the workload_type input parameter
  workload_types:
    test:
      description: Fast jobs (30-60 seconds)
      duration_range: "30-60s"
    light:
      description: Light workload (2-3 min)
      duration_range: "2-3min"
    standard:
      description: Standard workload (3-5 min)
      duration_range: "3-5min"
    heavy:
      description: Heavy workload (5-8 min)
      duration_range: "5-8min"

  # Default inputs applied to all workflow dispatches
  default_inputs:
    enable_randomization: true

# Test profiles - all use the primary workflow defined above
# To change the workflow, update workflows.primary.file - no need to edit each profile
test_profiles:
  # Validation test - 5 minutes with fast jobs (30-60 seconds each)
  validation:
    duration_minutes: 5
    dispatch_pattern: steady
    jobs_per_minute: 1.0
    workload_inputs:
      workload_type: test

  # Performance test with fast workload - 10 minutes
  performance_fast:
    duration_minutes: 10
    dispatch_pattern: steady
    jobs_per_minute: 1.2
    workload_inputs:
      workload_type: test

  # Quick test - 5 minutes
  quick:
    duration_minutes: 5
    dispatch_pattern: steady
    jobs_per_minute: 1.0

  # Performance baseline test
  performance:
    duration_minutes: 30
    dispatch_pattern: steady
    jobs_per_minute: 1.2      # 72 jobs/hour

  # Capacity test (fast) - validate capacity testing with queuing
  capacity_fast:
    duration_minutes: 12
    dispatch_pattern: steady
    jobs_per_minute: 3.0      # 3 jobs/min causes queuing
    workload_inputs:
      workload_type: test

  # Capacity test - maximum throughput
  capacity:
    duration_minutes: 60
    dispatch_pattern: steady
    jobs_per_minute: 1.0
    workload_inputs:
      workload_type: standard

  # Stress test (fast) - validate burst handling
  stress_fast:
    duration_minutes: 5
    dispatch_pattern: burst
    burst_size: 8             # Send 8 jobs at once (double capacity)
    burst_interval: 150       # Every 2.5 minutes
    workload_inputs:
      workload_type: test

  # Stress test - sudden load
  stress:
    duration_minutes: 45
    dispatch_pattern: burst
    burst_size: 8
    burst_interval: 900       # Every 15 minutes
    workload_inputs:
      workload_type: standard

  # Load test (fast) - validate sustained load behavior
  load_fast:
    duration_minutes: 10
    dispatch_pattern: steady
    jobs_per_minute: 0.8      # Below capacity - no queuing
    workload_inputs:
      workload_type: test

  # Load test - sustained load
  load:
    duration_minutes: 120
    dispatch_pattern: steady
    jobs_per_minute: 0.8
    workload_inputs:
      workload_type: standard

  # Spike test (medium) - demonstrates spike with queuing and recovery
  spike_medium:
    duration_minutes: 15
    dispatch_pattern: spike
    normal_rate: 0.5
    spike_rate: 4.0
    spike_duration: 300
    spike_start: 180
    workload_inputs:
      workload_type: test

  # Spike test - sudden traffic spike
  spike:
    duration_minutes: 60
    dispatch_pattern: spike
    normal_rate: 0.2
    spike_rate: 2.0
    spike_duration: 600
    spike_start: 1200
    workload_inputs:
      workload_type: standard

# Metrics to collect
metrics:
  collection_interval: 30     # Collect metrics every 30 seconds

  github_api:
    enabled: true
    endpoints:
      - workflow_runs
      - jobs
      - runners

  custom_metrics:
    - queue_time            # Time job waits for runner
    - execution_time        # Time job takes to complete
    - runner_utilization    # % of time runners are busy
    - throughput           # Jobs completed per hour
    - failure_rate         # % of failed jobs

# Reporting configuration
reporting:
  format: json
  output_directory: test_results/aws_ecs

  report_types:
    - summary              # Overall test summary
    - detailed            # Detailed metrics over time
    - comparison          # Compare with baseline

  visualization:
    enabled: true
    charts:
      - queue_time_histogram
      - runner_utilization_timeline
      - throughput_chart
      - job_duration_distribution

# Network configuration (if needed)
network:
  proxy:
    enabled: false

  ssl:
    verify: true

  timeout:
    api_call: 30
    workflow_dispatch: 60

# Logging configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: logs/aws_ecs_test.log
  console: true

  # Rotate logs
  rotation:
    max_bytes: 10485760    # 10MB
    backup_count: 5